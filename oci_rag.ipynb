{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_file, divide_docs, recover_doc, calculate_similarity\n",
    "import oci\n",
    "import pandas as pd\n",
    "from numpy import array, max\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding a document in a vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = oci.config.from_file('config', \"DEFAULT\") # Caminho do arquivo de configuração\n",
    "endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config,\n",
    "                                                                                         service_endpoint=endpoint,\n",
    "                                                                                         retry_strategy=oci.retry.NoneRetryStrategy(), \n",
    "                                                                                         timeout=(10,240)) # Configuração do client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml file with secrets using pyyaml\n",
    "with open(\"secrets.yaml\", 'r') as stream:\n",
    "\ttry:\n",
    "\t\tsecret = yaml.safe_load(stream)\n",
    "\texcept yaml.YAMLError as exc:\n",
    "\t\tprint(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query, generative_ai_inference_client): \n",
    "\tembed_text_detail = oci.generative_ai_inference.models.EmbedTextDetails()\n",
    "\tembed_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"cohere.embed-english-v3.0\") # Modelo a ser utilizado\n",
    "\tembed_text_detail.inputs = query\n",
    "\tembed_text_detail.input_type = \"SEARCH_QUERY\" # Tipo de input, no caso, uma query\n",
    "\tembed_text_detail.truncate = \"NONE\"\n",
    "\tembed_text_detail.compartment_id = secret['compartment_id']\n",
    "  \n",
    "\tembed_text_response = generative_ai_inference_client.embed_text(embed_text_detail)\n",
    "\treturn embed_text_response.data.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunks_to_df(df_docs, doc_title, doc_text):\n",
    "    '''\n",
    "    Function to add document's chunks to the dataframe\n",
    "    '''\n",
    "    chunks = divide_docs(doc_text)\n",
    "    n_chunk = len(chunks)\n",
    "    embeds = embed_query(chunks, generative_ai_inference_client)\n",
    "    for i in range(n_chunk):\n",
    "        df_docs.loc[len(df_docs.index)] = [doc_title, chunks[i], embeds[i], i+1]\n",
    "    return df_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all docs in docs folder and process them\n",
    "import os\n",
    "\n",
    "docs_folder = \"../docs/\"\n",
    "df_docs = pd.DataFrame([], columns=['doc_title', 'doc_text', 'doc_embed', 'n_chunk'])\n",
    "\n",
    "for file in os.listdir(docs_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        doc = read_file(docs_folder + file)\n",
    "        df_docs = add_chunks_to_df(df_docs, file, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_text</th>\n",
       "      <th>doc_embed</th>\n",
       "      <th>n_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OML Services - Deploy.txt</td>\n",
       "      <td>Deploy Oracle Machine Learning Models\\nOracle ...</td>\n",
       "      <td>[-0.009811401, -0.033966064, -0.035125732, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OML Services - Deploy.txt</td>\n",
       "      <td>ust send this token in the Authorization heade...</td>\n",
       "      <td>[0.01676941, -0.015113831, 5.930662e-05, -0.06...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OML Services - Deploy.txt</td>\n",
       "      <td>ment of a deployed model endpoint with URI km_...</td>\n",
       "      <td>[-0.0005927086, -0.069885254, 0.01576233, -0.0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overview of Generative AI Service.txt</td>\n",
       "      <td>Overview of Generative AI Service\\nOracle Clou...</td>\n",
       "      <td>[-0.018096924, -0.047912598, -0.012390137, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overview of Generative AI Service.txt</td>\n",
       "      <td>which the fine-tuned model is derived from.\\n\\...</td>\n",
       "      <td>[-0.017562866, -0.026641846, -0.02432251, -0.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Overview of Generative AI Service.txt</td>\n",
       "      <td>s, for example:\\n\\nEvaluate a list of question...</td>\n",
       "      <td>[0.00844574, -0.033172607, -0.008522034, -0.04...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pretrained Foundational Models in Generative A...</td>\n",
       "      <td>Pretrained Foundational Models in Generative A...</td>\n",
       "      <td>[-0.009719849, -0.021118164, -0.014373779, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pretrained Foundational Models in Generative A...</td>\n",
       "      <td>ish or multilingual.\\nModel creates a 384-dime...</td>\n",
       "      <td>[-0.04537964, -0.012008667, -0.033569336, -0.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pretrained Image Analysis Models.txt</td>\n",
       "      <td>Pretrained Image Analysis Models\\nVision provi...</td>\n",
       "      <td>[0.059417725, 0.0031318665, 0.008033752, -0.02...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pretrained Image Analysis Models.txt</td>\n",
       "      <td>the objects and identifies them.\\n\\nVision pr...</td>\n",
       "      <td>[0.010192871, -0.003168106, -0.031311035, -0.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pretrained Image Analysis Models.txt</td>\n",
       "      <td>nguage of a document, then OCR draws bounding ...</td>\n",
       "      <td>[-0.025360107, -0.008232117, -0.004760742, -0....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Speech Overview.txt</td>\n",
       "      <td>Speech Overview\\nYou can use the Speech servic...</td>\n",
       "      <td>[-0.050720215, -0.02960205, -0.05065918, -0.06...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Speech Overview.txt</td>\n",
       "      <td>z\\nFrenchâ€”French\\tfr-FR\\t&gt;= 16 khz\\nGermanâ€...</td>\n",
       "      <td>[-0.01651001, -0.021087646, -0.011131287, -0.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Speech Overview.txt</td>\n",
       "      <td>bs that aren't yet processed saving time and m...</td>\n",
       "      <td>[0.030700684, -0.023071289, -0.033569336, -0.1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Speech Overview.txt</td>\n",
       "      <td>process.\\n\\nAuthentication and Authorization\\...</td>\n",
       "      <td>[-0.01737976, -0.013977051, -0.045837402, -0.0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Speech Overview.txt</td>\n",
       "      <td>s guide.\\n\\nTo access the Console, you must us...</td>\n",
       "      <td>[-0.03326416, -0.037322998, -0.02420044, -0.02...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Supported Features and Examples.txt</td>\n",
       "      <td>Supported Features and Examples - Oracle Machi...</td>\n",
       "      <td>[0.033996582, -0.017486572, 0.0021820068, -0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Supported Features and Examples.txt</td>\n",
       "      <td>\\nFor more information, refer to:\\n- Work with...</td>\n",
       "      <td>[0.01713562, -0.008644104, -0.002708435, -0.04...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Supported Features and Examples.txt</td>\n",
       "      <td>erform various actions such as update, enable,...</td>\n",
       "      <td>[0.014511108, -0.022857666, 0.013450623, -0.03...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Embedding Models.txt</td>\n",
       "      <td>The Embedding Models\\nAn embedding is a numeri...</td>\n",
       "      <td>[0.024261475, -0.017822266, 0.0019435883, -0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Embedding Models.txt</td>\n",
       "      <td>\\nTruncate\\nWhether to truncate the start or e...</td>\n",
       "      <td>[-0.002609253, -0.02255249, 0.021728516, -0.01...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Generation Models.txt</td>\n",
       "      <td>The Generation Models\\nAvailable pretrained fo...</td>\n",
       "      <td>[-0.032043457, -0.004016876, -0.02960205, -0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Generation Models.txt</td>\n",
       "      <td>fine-tune the output by changing the followin...</td>\n",
       "      <td>[-0.031280518, -0.015129089, 0.007736206, -0.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Generation Models.txt</td>\n",
       "      <td>ne stop sequence, then the model stops when it...</td>\n",
       "      <td>[0.050994873, -0.0129852295, 0.015426636, -0.0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Summarization Models.txt</td>\n",
       "      <td>The Summarization Models\\nFor the summarizatio...</td>\n",
       "      <td>[-0.008026123, -0.0019073486, -0.023483276, -0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Summarization Models.txt</td>\n",
       "      <td>s, a recommended temperature value is 0.2. Use...</td>\n",
       "      <td>[0.02911377, 0.025054932, 0.0022201538, -0.030...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Vision Service Overview.txt</td>\n",
       "      <td>Service Overview\\nOracle Cloud Infrastructure ...</td>\n",
       "      <td>[-0.006034851, -0.052459717, -0.023147583, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Vision Service Overview.txt</td>\n",
       "      <td>Recognition (OCR): Vision can detect and reco...</td>\n",
       "      <td>[-0.01625061, -0.028823853, -0.03933716, -0.06...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Vision Service Overview.txt</td>\n",
       "      <td>port)\\nUS East (Ashburn)\\nUS West (Phoenix)\\nU...</td>\n",
       "      <td>[-0.016967773, -0.032684326, -0.015213013, -0....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_title  \\\n",
       "0                           OML Services - Deploy.txt   \n",
       "1                           OML Services - Deploy.txt   \n",
       "2                           OML Services - Deploy.txt   \n",
       "3               Overview of Generative AI Service.txt   \n",
       "4               Overview of Generative AI Service.txt   \n",
       "5               Overview of Generative AI Service.txt   \n",
       "6   Pretrained Foundational Models in Generative A...   \n",
       "7   Pretrained Foundational Models in Generative A...   \n",
       "8                Pretrained Image Analysis Models.txt   \n",
       "9                Pretrained Image Analysis Models.txt   \n",
       "10               Pretrained Image Analysis Models.txt   \n",
       "11                                Speech Overview.txt   \n",
       "12                                Speech Overview.txt   \n",
       "13                                Speech Overview.txt   \n",
       "14                                Speech Overview.txt   \n",
       "15                                Speech Overview.txt   \n",
       "16                Supported Features and Examples.txt   \n",
       "17                Supported Features and Examples.txt   \n",
       "18                Supported Features and Examples.txt   \n",
       "19                           The Embedding Models.txt   \n",
       "20                           The Embedding Models.txt   \n",
       "21                          The Generation Models.txt   \n",
       "22                          The Generation Models.txt   \n",
       "23                          The Generation Models.txt   \n",
       "24                       The Summarization Models.txt   \n",
       "25                       The Summarization Models.txt   \n",
       "26                        Vision Service Overview.txt   \n",
       "27                        Vision Service Overview.txt   \n",
       "28                        Vision Service Overview.txt   \n",
       "\n",
       "                                             doc_text  \\\n",
       "0   Deploy Oracle Machine Learning Models\\nOracle ...   \n",
       "1   ust send this token in the Authorization heade...   \n",
       "2   ment of a deployed model endpoint with URI km_...   \n",
       "3   Overview of Generative AI Service\\nOracle Clou...   \n",
       "4   which the fine-tuned model is derived from.\\n\\...   \n",
       "5   s, for example:\\n\\nEvaluate a list of question...   \n",
       "6   Pretrained Foundational Models in Generative A...   \n",
       "7   ish or multilingual.\\nModel creates a 384-dime...   \n",
       "8   Pretrained Image Analysis Models\\nVision provi...   \n",
       "9    the objects and identifies them.\\n\\nVision pr...   \n",
       "10  nguage of a document, then OCR draws bounding ...   \n",
       "11  Speech Overview\\nYou can use the Speech servic...   \n",
       "12  z\\nFrenchâ€”French\\tfr-FR\\t>= 16 khz\\nGermanâ€...   \n",
       "13  bs that aren't yet processed saving time and m...   \n",
       "14   process.\\n\\nAuthentication and Authorization\\...   \n",
       "15  s guide.\\n\\nTo access the Console, you must us...   \n",
       "16  Supported Features and Examples - Oracle Machi...   \n",
       "17  \\nFor more information, refer to:\\n- Work with...   \n",
       "18  erform various actions such as update, enable,...   \n",
       "19  The Embedding Models\\nAn embedding is a numeri...   \n",
       "20  \\nTruncate\\nWhether to truncate the start or e...   \n",
       "21  The Generation Models\\nAvailable pretrained fo...   \n",
       "22   fine-tune the output by changing the followin...   \n",
       "23  ne stop sequence, then the model stops when it...   \n",
       "24  The Summarization Models\\nFor the summarizatio...   \n",
       "25  s, a recommended temperature value is 0.2. Use...   \n",
       "26  Service Overview\\nOracle Cloud Infrastructure ...   \n",
       "27   Recognition (OCR): Vision can detect and reco...   \n",
       "28  port)\\nUS East (Ashburn)\\nUS West (Phoenix)\\nU...   \n",
       "\n",
       "                                            doc_embed  n_chunk  \n",
       "0   [-0.009811401, -0.033966064, -0.035125732, -0....        1  \n",
       "1   [0.01676941, -0.015113831, 5.930662e-05, -0.06...        2  \n",
       "2   [-0.0005927086, -0.069885254, 0.01576233, -0.0...        3  \n",
       "3   [-0.018096924, -0.047912598, -0.012390137, -0....        1  \n",
       "4   [-0.017562866, -0.026641846, -0.02432251, -0.0...        2  \n",
       "5   [0.00844574, -0.033172607, -0.008522034, -0.04...        3  \n",
       "6   [-0.009719849, -0.021118164, -0.014373779, -0....        1  \n",
       "7   [-0.04537964, -0.012008667, -0.033569336, -0.0...        2  \n",
       "8   [0.059417725, 0.0031318665, 0.008033752, -0.02...        1  \n",
       "9   [0.010192871, -0.003168106, -0.031311035, -0.0...        2  \n",
       "10  [-0.025360107, -0.008232117, -0.004760742, -0....        3  \n",
       "11  [-0.050720215, -0.02960205, -0.05065918, -0.06...        1  \n",
       "12  [-0.01651001, -0.021087646, -0.011131287, -0.0...        2  \n",
       "13  [0.030700684, -0.023071289, -0.033569336, -0.1...        3  \n",
       "14  [-0.01737976, -0.013977051, -0.045837402, -0.0...        4  \n",
       "15  [-0.03326416, -0.037322998, -0.02420044, -0.02...        5  \n",
       "16  [0.033996582, -0.017486572, 0.0021820068, -0.0...        1  \n",
       "17  [0.01713562, -0.008644104, -0.002708435, -0.04...        2  \n",
       "18  [0.014511108, -0.022857666, 0.013450623, -0.03...        3  \n",
       "19  [0.024261475, -0.017822266, 0.0019435883, -0.0...        1  \n",
       "20  [-0.002609253, -0.02255249, 0.021728516, -0.01...        2  \n",
       "21  [-0.032043457, -0.004016876, -0.02960205, -0.0...        1  \n",
       "22  [-0.031280518, -0.015129089, 0.007736206, -0.0...        2  \n",
       "23  [0.050994873, -0.0129852295, 0.015426636, -0.0...        3  \n",
       "24  [-0.008026123, -0.0019073486, -0.023483276, -0...        1  \n",
       "25  [0.02911377, 0.025054932, 0.0022201538, -0.030...        2  \n",
       "26  [-0.006034851, -0.052459717, -0.023147583, -0....        1  \n",
       "27  [-0.01625061, -0.028823853, -0.03933716, -0.06...        2  \n",
       "28  [-0.016967773, -0.032684326, -0.015213013, -0....        3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Image Analysis Models\n",
      "Vision provides pretrained image analysis AI models that let you to find and tag objects, text, and entire scenes in images.\n",
      "\n",
      "Pretrained models let you use AI with no data science experience. Provide an image to the Vision service and get back information about the objects, text, scenes, and any faces in the image without having to create your own model.\n",
      "\n",
      "Use Cases\n",
      "Here are several use cases for pretrained image analysis models.\n",
      "\n",
      "Digital asset management\n",
      "Tag digital media-like images for better indexing and retrieval.\n",
      "Scene monitoring\n",
      "Detect if items are on retail shelves, vegetation is growing in the surveillance image of a power line, or if trucks are available at a lot for delivery or shipment.\n",
      "Face detection\n",
      "Privacy: Hide identities by adding a blur to the image using face location information returned through the face detection feature.\n",
      "Prerequisite for Biometrics: Use the facial quality score to decide if a face is clear and unobstructed.\n",
      "Digital asset management: Tag images with facial information for better indexing and retrieval.\n",
      "Supported Formats\n",
      "Vision supports several image analysis formats.\n",
      "\n",
      "Images can be uploaded either from local storage or Oracle Cloud Infrastructure Object Storage. The images can be in the following formats:\n",
      "JPG\n",
      "PNG\n",
      "Pretrained Models\n",
      "Vision has four types of pretrained image analysis model.\n",
      "\n",
      "The pretrained models are:\n",
      "Object Detection\n",
      "Image Classification\n",
      "Face Detection\n",
      "Optical Character Recognition (OCR)\n",
      "Object Detection\n",
      "Object detection is used to locate and identity objects within an image. For example, if you have an image of a living room, Vision locates the objects there, such as a chair, a sofa, and a TV. It then provides bounding boxes for each of the objects and identifies them.\n",
      "\n",
      "Vision provides a confidence score for each object identified. The confidence score is a decimal number. Scores closer to 1 indicate a higher confidence in the objects classification, while lower scores indicate a lower confidence score. The range of the confidence score for each label is from 0 to 1.\n",
      "\n",
      "Supported features are:\n",
      "Labels\n",
      "Confidence score\n",
      "Object-bounding polygons\n",
      "Single requests\n",
      "Batch requests\n",
      "Object Detection Example\n",
      "Image Classification\n",
      "Image classification can be used to identify scene-based features and objects in an image. You can have one classification or many classifications, depending on the use case and the number of items in an image. For example, if you have an image of a person running, Vision identifies the person, the clothing, and the footwear.\n",
      "\n",
      "Vision provides a confidence score for each label. The confidence score is a decimal number. Scores closer to 1 indicate a higher confidence in the label, while lower scores indicate lower confidence score. The range of the confidence score for each label is from 0 to 1.\n",
      "\n",
      "Supported features are:\n",
      "Labels\n",
      "Confidence score\n",
      "Ontology classes\n",
      "Single requests\n",
      "Batch requests\n",
      "Image Classification Example\n",
      "Face Detection\n",
      "Vision can detect and recognize faces in an image.\n",
      "\n",
      "Face detection lets you pass an image or a batch of images to Vision to detect the following using a pretrained model:\n",
      "\n",
      "The existence of faces in each image.\n",
      "The location of faces in each image.\n",
      "Face landmarks for each face.\n",
      "Visual quality of each face.\n",
      "No data science experience is required to use this pretrained model.\n",
      "\n",
      "Face Example\n",
      "Optical Character Recognition (OCR)\n",
      "Vision can detect and recognize text in a document.\n",
      "\n",
      "Language classification identifies the language of a document, then OCR draws bounding boxes around the printed or hand-written text it locates in an image, and digitizes the text. For example, if you have an image of a stop sign, Vision locates the text in that image and extracts the text STOP. It provides bounding boxes for the identified text.\n",
      "\n",
      "Vision provides a confidence score for each text grouping. The confidence score is a decimal number. Scores closer to 1 indicate a higher confidence in the extracted text, while lower scores indicate lower confidence score. The range of the confidence score for each label is from 0 to 1.\n",
      "\n",
      "Text Detection can be used with Document AI or Image Analysis models.\n",
      "\n",
      "OCR support is limited to English. If you know the text in your images is in English, set the language to Eng.\n",
      "\n",
      "Supported features are:\n",
      "Word extraction\n",
      "Text line extraction\n",
      "Confidence score\n",
      "Boundling polygons\n",
      "Single request\n",
      "Batch request\n",
      "OCR Example\n",
      "Using the Pretrained Image Analysis Models\n",
      "Vision provides pretrained models for customers to extract insights about their images without needing Data Scientists.\n",
      "\n",
      "You need the following before using a pretrained model:\n",
      "\n",
      "A paid tenancy account in Oracle Cloud Infrastructure.\n",
      "\n",
      "Familiarity with Oracle Cloud Infrastructure Object Storage.\n",
      "\n",
      "You can call the pretrained Image Analysis models as a batch request using Rest APIs, SDK, or CLI. You can call the pretrained Image Analysis models as a single request using the Console, Rest APIs, SDK, or CLI.\n",
      "\n",
      "See the Limits section for information on what is allowed in batch requests.\n"
     ]
    }
   ],
   "source": [
    "print(recover_doc(df_docs,'Pretrained Image Analysis Models.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the similarity between query and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please help me with iot stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed query and calculate similarity\n",
    "query_embedding = embed_query([query], generative_ai_inference_client)\n",
    "embeds_array = array(df_docs['doc_embed'])\n",
    "similarities = [calculate_similarity(query_embedding, embedding) for embedding in embeds_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Overview.txt Similaty Score: 0.289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Speech Overview\\nYou can use the Speech service to convert media files to readable text that's stored in JSON and SRT format.\\n\\nSpeech harnesses the power of spoken language enabling you to easily convert media files containing human speech into highly exact text transcriptions. The service is an Oracle Cloud Infrastructure (OCI) native application that you can access using the Console, REST API, CLI, and SDK. In addition, you can use the Speech service in a Data Science notebook session.\\n\\nSpeech uses automatic speech recognition (ASR) technology to provide a grammatically correct transcription. Speech handles low-fidelity media recordings and transcribes challenging recordings such as meetings or call centers calls. Using Speech, you can turn files stored in Object Storage or a data asset into exact, normalized, timestamped, and profanity-filtered text. This functionality is only available with the Speech. For example, you could index the output of speech (a text file) using Data Lake. Without the downstream services, this capability doesn't exist in Speech.\\n\\nShows the speech engine process, media to front-end, to backend to results.\\nThe Speech models are robust to acoustic environments and recording channels that ensure that this is a good quality transcription service.\\n\\nMultiple Media Format Support per Language\\nThese media formats are supported for all supported languages in the Speech service:\\n\\nAAC\\nAC3\\nAMR\\nAU\\nFLAC\\nM4A\\nMKV\\nMP3\\nMP4\\nOGA\\nOGG\\nWAV\\nWEBM\\nLanguage\\tLanguage Code\\tSample Rate\\nEnglishâ€”United States\\ten-US\\t>= 8 khz\\nSpanishâ€”Spain\\tes-ES\\t>= 8 khz\\nPortugueseâ€”Brazil\\tpt-BR\\t>= 8 khz\\nEnglishâ€”Great Britain\\ten-GB\\t>= 16 khz\\nEnglishâ€”Australia\\ten-AU\\t>= 16 khz\\nEnglishâ€”India\\ten-IN\\t>= 16 khz\\nHindiâ€”India\\thi-IN\\t>= 16 khz\\nFrenchâ€”French\\tfr-FR\\t>= 16 khz\\nGermanâ€”Germany\\tde-DE\\t>= 16 khz\\nItalianâ€”Italy\\tit-IT\\t>= 16 khz\\nFor best results:\\n\\nUse a lossless format such as FLAC or WAV with PCM 16-bit encoding.\\nUse a sample rate of 8, 000 Hz for low-fidelity media and 16,000 to 48, 000 Hz for high fidelity media.\\nYou can use single-channel, 16-bit PCM WAV media files with an 8 kHz or 16 kHz sample rate. We recommend Audacity (GUI) or FFmpeg (command line) for media transcoding. A maximum media file length of four hours and up to 2 GB is supported.\\n\\nSpeech is susceptible to the quality of the input media files. Different accents, background noises, switching from one language to another, using fusion languages, or multiple speakers at the same time impact the quality of the transcription.\\n\\nSpeech Provides These Capabilities\\nAccurate transcriptionsâ€”Produces an accurate and easy to use JSON and SubRip Subtitle (SRT) files written directly to the Object Storage bucket you choose. You can take advantage of the transcription and integrate it directly with applications, and use it for subtitles or content search and analysis.\\n\\nWhisper modelâ€”Multilingual data is collected from the web and supports file based voice to text transcription for 50+ languages.\\nTime stamped JSONâ€”The transcription provides a timestamp for each token (word). You can use the timestamp to search and find the text you're looking for within the media file then quickly jump to that location.\\n\\nMultilingualâ€”Produces accurate transcriptions in English, English-Great Britain, English-Australia, English-India, Spanish, Portuguese, French, Italian, German, and Hindi.\\n\\nAsynchronous APIâ€”Straightforward asynchronous APIs with transcription task batching. The APIs enable canceling jobs that aren't yet processed saving time and money.\\n\\nText normalizationsâ€”Provides text normalizations for numbers, addresses, currencies, and so on. With text normalizations, you get a higher-quality transcription from artificial intelligence that's easier to read and understand.\\n\\nProfanity filteringâ€”Allows you to remove, mask, or tag words that are offensive from the transcription.\\n\\nConfidence score per word and transcriptionâ€”Produces word and transcription confidence scores on the generated JSON file. You can use the confidence scores to quickly identify words that require attention.\\n\\nClosed captionsâ€”Provides you with an SRT file as an extra output format. Use the SRT to add closed captions to video files.\\n\\nPunctuationâ€”Long text requires punctuation so Speech punctuates the transcribe content automatically.\\n\\nTelephoney readyâ€”Files can be 8 kHz or 16 kHz and each are automatically detected so that the correct model is applied. With this capability, you can transcribe telephone recordings.\\n\\nKey Concepts\\nThese are the key Speech service concepts:\\n\\nTranscription Jobs\\nA job is a single asynchronous request from the Console or the Speech API. Each job is uniquely identified by an id, which you can use to retrieve job status and results.\\n\\nA job in a tenant is processed in a strict first in first out manner. Each job can contain up to 100 tasks. If you submit a job that exceeds the maximum tasks, that job fails. Jobs are retained for 90 days.\\n\\nTasks\\nA task is the result of a single file processed in a job. Jobs can have multiple tasks based on what's stored in your Object Storage bucket that you specify for a job.\\n\\nModels\\nPretrained acoustic and language models, including Whisper models, power the job transcription process.\\n\\nAuthentication and Authorization\\nEach service in OCI integrates with IAM for authentication and authorization, for all interfaces (the Console, SDK or CLI, and REST API).\\n\\nAn administrator in your organization needs to set up groups , compartments , and policies  that control which users can access which services, which resources, and the type of access. For example, the policies control who can create new users, create and manage the cloud network, launch instances, create buckets, download objects, and so on. For more information, see Getting Started with Policies.\\n\\nFor details about writing Speech policies, see About Speech Policies.\\nFor details about writing policies for other services, see Policy Reference.\\nIf you're a regular user (not an administrator) who needs to use the OCI resources that your company owns, contact your administrator to set up a user ID for you. The administrator can confirm which compartment or compartments you should be using.\\n\\nResource Identifiers\\nThe Speech service supports jobs and tasks as OCI resources. Most types of resources have a unique, Oracle-assigned identifier called an Oracle Cloud ID (OCID). For information about the OCID format and other ways to identify your resources, see Resource Identifiers.\\n\\nRegions and Availability Domains\\nSpeech is available in all OCI commercial regions. See About Regions and Availability Domains for the list of available regions for OCI, along with associated locations, region identifiers, region keys, and availability domains.\\n\\nWays to Access\\nYou can access Speech using the Console (a browser-based interface), the command line interface (CLI), or the REST API. Instructions for the Console, CLI, and API are included in topics throughout this guide.\\n\\nTo access the Console, you must use a supported browser. To go to the Console sign-in page, open the navigation menu at the top of this page and click Infrastructure Console. You are prompted to enter your cloud tenant, your user name, and your password.\\n\\nFor a list of available SDKs, see SDKs and the CLI. For general information about using the APIs, see REST API.\\n\\nService Limits\\nIn each region that is enabled for your tenancy, these limits apply:\\n\\nFile Limits\\n\\nThe maximum file size is 2 GB.\\n\\nFile duration is a maximum of 4 hours.\\n\\nJob Limits\\n\\nEach job can have up to 100 tasks.\\n\\nJobs are retained for 90 days.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the most similar document\n",
    "max_similarity = max(similarities)\n",
    "most_similar_doc = df_docs.iloc[similarities.index(max_similarity)]\n",
    "print(most_similar_doc['doc_title'], \"Similaty Score:\", round(max_similarity,3))\n",
    "recover_doc(df_docs, most_similar_doc['doc_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model_id, generative_ai_inference_client = generative_ai_inference_client):\n",
    "    '''\n",
    "    Function to generate text based on a prompt and a LLM\n",
    "    '''\n",
    "    generate_text_detail = oci.generative_ai_inference.models.GenerateTextDetails()\n",
    "    llm_inference_request = oci.generative_ai_inference.models.CohereLlmInferenceRequest()\n",
    "    llm_inference_request.prompt = prompt\n",
    "    llm_inference_request.max_tokens = 1024\n",
    "    llm_inference_request.temperature = 0.11\n",
    "    llm_inference_request.frequency_penalty = 0\n",
    "    llm_inference_request.top_p = 0.2\n",
    "\n",
    "    generate_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)\n",
    "    generate_text_detail.inference_request = llm_inference_request\n",
    "    generate_text_detail.compartment_id = secret['compartment_id']\n",
    "    generate_text_response = generative_ai_inference_client.generate_text(generate_text_detail)\n",
    "\n",
    "    return generate_text_response.data.inference_response.generated_texts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceyafhwal37hxwylnpbcncidimbwteff4xha77n5xz4m7p6a\"\n",
    "prompt = \"Please help me with iot stuff\"\n",
    "ans = generate_text(prompt, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'd be happy to help you with Internet of Things (IoT) related concepts and questions. Feel free to ask specific questions or share the area of IoT you'd like to explore further. Here's a general overview of IoT to get us started:\n",
      "\n",
      "The Internet of Things (IoT) refers to the interconnection of devices, vehicles, buildings, and other physical objects that are embedded with electronics, software, sensors, and network connectivity, enabling them to collect and exchange data. This network of physical objects, connected to the internet, can communicate and collaborate with each other and with human users via smart devices.\n",
      "\n",
      "Here are some key aspects and components of IoT:\n",
      "\n",
      "1. **Devices**: These are the physical objects or systems that are connected to the internet. Examples include smart home devices like light bulbs, thermostats, and security cameras; industrial sensors and equipment; healthcare devices like wearables and medical devices; and many more. These devices often have embedded sensors that collect data from the physical world.\n",
      "\n",
      "2. **Sensors**: Sensors play a crucial role in IoT devices by converting physical phenomena into digital data. They can measure various parameters such as temperature, humidity, pressure, motion, light, and more. The data collected by these sensors is then transmitted over the internet.\n",
      "\n",
      "3. **IoT Platforms**: These are the backbone of IoT systems and provide the infrastructure for managing, monitoring, and controlling IoT devices and the data they generate. IoT platforms offer features like device management, data storage, analytics, and visualization. They help in integrating different components of an IoT ecosystem.\n",
      "\n",
      "4. **Network Communication**: IoT devices need to communicate with each other and with the cloud. This is typically done through wireless networks such as Wi-Fi, Bluetooth, or Low Power Wide Area Networks (LPWAN) like LoRaWAN or Narrowband IoT (NB-IoT). These networks enable efficient and reliable data transmission.\n",
      "\n",
      "5. **Data and Analytics**: IoT generates vast amounts of data, from sensor readings to device behavior. Analyzing this data can provide valuable insights for various industries and use cases. IoT analytics can help in predictive maintenance, resource optimization, fraud detection, and making data-driven decisions.\n",
      "\n",
      "6. **Cloud Computing**: Cloud platforms play a significant role in IoT by providing scalable infrastructure for data storage, processing power, and software services. Cloud providers like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud offer specific IoT services to support IoT applications.\n",
      "\n",
      "7. **Security and Privacy**: With the increasing number of connected devices, IoT security has become a critical concern. Securing IoT devices, networks, and data is essential to prevent unauthorized access, data breaches, and other security threats. \n",
      "\n",
      "IoT finds applications in various sectors, including industrial automation, smart cities, agriculture, transportation, healthcare, and consumer electronics. \n",
      "\n",
      "Would you like to know more about any of these topics or have a specific question related to IoT? Feel free to let me know, and I'll be glad to provide more detailed information and explanations. Additionally, if you have a particular use case or application in mind, we can explore it further and discuss the relevant aspects of IoT for that specific scenario. \n",
      "\n",
      "Please let me know if you'd like to dive deeper into any of these topics or if you have further questions related to IoT. \n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge(query, df_docs, generative_ai_inference_client, \n",
    "                  threshold=0.45, verbose=False):\n",
    "    '''\n",
    "    Function to get the most similar document to a query\n",
    "    '''\n",
    "    query_embedding = embed_query([query], generative_ai_inference_client)\n",
    "    embeds_array = array(df_docs['doc_embed'])\n",
    "    similarities = [calculate_similarity(query_embedding, embedding) for embedding in embeds_array]\n",
    "    max_similarity = max(similarities)\n",
    "    most_similar_doc = df_docs.iloc[similarities.index(max_similarity)]\n",
    "    if verbose:\n",
    "        print(most_similar_doc['doc_title'], \"Similaty Score:\", round(max_similarity,3))\n",
    "    if max_similarity < threshold:\n",
    "        return None\n",
    "    else:\n",
    "        return recover_doc(df_docs, most_similar_doc['doc_title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query, df_docs, model_id, generative_ai_inference_client, \n",
    "               threshold=.45, verbose=False):\n",
    "    '''\n",
    "    Function to get the answer to a query\n",
    "    '''\n",
    "    most_similar_doc = get_knowledge(query, df_docs, generative_ai_inference_client,\n",
    "                                     threshold, verbose)\n",
    "    if most_similar_doc is None:\n",
    "        return \"I'm sorry, I don't have the information in my database.\"\n",
    "    else:\n",
    "        prompt = most_similar_doc + \"\\n\\n\" + query\n",
    "        return generate_text(prompt, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which services are avaliable in the OCI vision API?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Service Overview.txt Similaty Score: 0.633\n",
      " The Oracle Cloud Infrastructure (OCI) Vision API provides two main categories of services: Document AI and Image Analysis. \n",
      "\n",
      "Document AI:\n",
      "- Optical Character Recognition (OCR)\n",
      "- Document Classification\n",
      "- Language Classification\n",
      "- Table Extraction\n",
      "- Key-Value Extraction\n",
      "\n",
      "Image Analysis:\n",
      "- Object Detection\n",
      "- Image Classification\n",
      "- Face Detection\n",
      "- Custom Object Detection\n",
      "- Custom Image Classification\n",
      "\n",
      "It's important to note that according to the provided documentation, some of these features are set to be moved to a new service called Document Understanding, and will only be available in Vision until January 1, 2024. After this date, they will be available exclusively in Document Understanding. \n",
      "\n",
      "Please refer to the official Oracle Cloud Infrastructure Vision API documentation for the most up-to-date information regarding available services and any service changes. \n",
      "\n",
      "Would you like me to help you with anything else regarding the OCI Vision API? \n"
     ]
    }
   ],
   "source": [
    "print(get_answer(query, df_docs, model_id, generative_ai_inference_client, \n",
    "                 threshold=0.4, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
